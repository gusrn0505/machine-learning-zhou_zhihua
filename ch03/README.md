# Ch03. 선형 모델

> | 발표자 | 황세현 |
> | --- | --- |

### 선형모델이란?

d개의 속성을 가진 샘플이 있고, 이러한 속성들의 선형 조합을 통해 예측하는 함수를 학습하는 모델

## 1. 기본 형식

- $ f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$

- $ f(x)=w^T+b $  

$ w$는 예측에 있어서 각 속성의 중요성(이해가능성)을 직관적으로 드러낸다

###### -> w의 절댓값이 클수록 중요성이 크다.

> 예시- 수박 분류 문제
> 
> $ f (x)= 0.2 x_색+0.5x_꼭+0.3x_소+1 $
> 
> 꼭지 > 소리 > 색 순으로 중요도 

### 

## 2. 선형 회귀

#### 선형회귀란?

회귀 모형의 한 유형으로서, 입력 특성의 선형 조합으로부터 연속 값을 출력하는 회귀 모델을 말한다.

- **단순 선형회귀**
  
  입력 속성이 하나뿐인 상황
  
  #### $f(x) = wx_i+b$
* **다중 선형회귀**
  
  입력 속성이 여러개인 상황
  
  #### $ f(x) = w^Tx_i+b$

##### * 속성 변환

1. 순서가 있는 경우
   
   > **연속값**으로 변환
   > 
   > 예시) '크다', '작다'->{1.0, 0.0}
   > 
   >           '높음', '중간', '낮음' -> {1.0, 0.5, 0.0}

2. 순서가 없는 경우
   
   > **K개의 백터**로 변환
   > 
   > 예시) '수박', '호박', '오이' ->(0,0,1), (0,1,0), (1,0,0)

#### $ w$와 $ b$의 결정방법

* #### 최소제곱법
  
  평균제곱 오차를 최소화하는 방법
  
  > 평균제곱 오차를 최소화하는 과정을 **최소제곱 파라미터 예측**이라고 한다.
  
  **-> 샘플과 유클리드 거리의 합이 가장 작은 하나의 직선을 찾는 것이 목표**

##### 1) 단순 선형 회귀인 경우

  **-평균제곱 오차**

#### $ (w*,b*) = argmin\sum_{i=1}^m (f(x_i)-y_i)^2 $

###### -최소제곱 파라미터 예측

  w와 b에 관한 컨백스 함수가 w와 b에 대하여 **도함수가 0** 일 때, w와 b의 최적해를 구할 수 있다.

> 컨백스 함수란 볼록 함수를 의미

###### 1. 편미분

  $ \partial E\over\partial w $ $ =2(w\sum_{i=1} ^m x_i^2-\sum_{i=1} ^m (y_i-b)x_i)$

  $ \partial E\over\partial b$ $ =2(mb-\sum_{i=1} ^m (y_i-wx_i))$

###### 2. 도함수가 0일 때 w,b의 값

<img title="" src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-21-33-21-image.png" alt="" width="205">

> $\bar{x}$는 $ x$의 평균값을 의미

##### 2) 다중 선형 회귀인 경우

  **-가정**

1. $ \hat{w}=(w;b)$ : $ w$와 $ b$가 합쳐진 형태의 백터

2. 데이터 세트 D -> 행렬 X로 표현
   
   <img title="" src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-21-33-42-image.png" alt="" width="349">
   
   3.$ y=(y_1;y_2;y_3;...;y_m)$

**- 평균제곱 오차**

#### $ \hat{w}*= argmin(y-X\hat{w})^T(y-X\hat{w})$

  **-최소제곱 파라미터 예측**

  $ \hat{w}$의 컨백스 함수가 $\hat{w}$에 대하여 도함수가 0일 때, $ \hat{w}$의 최적해를 구할 수 있다.

###### 1. 편미분

  $ \partial E\over\partial \hat{w}$ $ =2X^T(X\hat{w}-y)$

###### 2. 도함수가 0일 때 $ \hat{w}$의 값

* $ X^TX$**가 풀랭크 행렬 또는 정치 행렬 일 때**
  
  $ \hat{w}*= (X^TX)X^{-1}X^Ty$
  
  > **풀랭크 행렬** 이란, 한 행에서 전부 선형 독립이거나 한 열에서 전부 선형 독립인 행렬을 말한다.
  > 
  > **정치 행렬** 이란, 임의의 열벡터 $ x$와 대칭행렬 A에 대해 $ x^TAx>0$이 성립하는 행렬을 말한다. 

* **그 외의 경우**
  
  현실에서는 풀랭크 행렬인 경우가 적다.
  
  (ex. 샘플보다 많은 변수를 가진 문제-> 백터 X의 **열의 수 > 행의 수**)
  
  -> **정규화 방법**을 사용해서 평균제곱 오차 최소화





## 3. 로지스틱 회귀

#### 로지스틱 회귀란?

  앞에서 선형 모델을 이용해 회귀 학습을 하는 방법에 대해 논하였다. 그러나 분류 문제에서 선형회귀모델이 생성한 예측값을 그대로 사용할 수 없다. 이 때 '**단위 계단 함수**' 를 활용하는데 단위 계단 함수는 불연속적이므로 **링크함수**를 직접적으로 사용할 수 없게 된다.  이 경우에 **로지스틱 함수**가 자주 쓰이고, 이 때 이용되는 회귀 학습이 **로지스틱 회귀**이다.

* 로지스틱 회귀의 명칭은 '회귀'지만 사실상 일종의 **분류학습법**

* 분류 가능성에 대해 직접적인 모델을 만듦으로 **사전 데이터 분포에 대한 가정이 필요X**

* **근사 확률에 대한 예측 가능**

* 로지스틱 회귀의 해의 목표함수는 **컨백스 함수** (최적해 구하기 용이)
  
  #### 로지스틱 함수
  
  ##### $ y=$ $ 1\over 1+e^{-z}$
  
  > 로지스틱 함수를 이용하면 z값이 0이나 1에 근사한 y값으로 바뀌고 z=0근처에서 결과값이 급격히 변한다.

##### -> 로지스틱 함수를 링크 함수로 대입

##### $ y=$ $ 1\over 1+e^{-w^Tx+b}$

  다음과 같이 바꿀 수 있다.

##### $ ln$ $ y\over 1-y$ $ =w^Tx +b$

> 만약 y를 x가 양성값일 가능성으로 본다면, 1-y는 음성값일 가능성이 되고 이 둘의 비굣값인 $ y\over 1-y$를 '**오즈**'라고 부른다.

#### w와 b의 결정 방법

- ##### 최대 우도법
  
  <img title="" src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic2.png" alt="최대우도법(MLE) - 공돌이의 수학정리노트" width="247"><img title="" src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic1.png" alt="최대우도법(MLE) - 공돌이의 수학정리노트" width="244">

> 각 데이터들의 확률밀도함수로 부터 얻는 기여도를 전부 곱한 것을 **우도함수**라고 한다. 이 때 기여도는 높이에 해당한다.
> 
> #### $ P(x\mid \theta) = \prod_{k=1} ^n P(x_k\mid \theta)$

###### : 최대 우도법은 우도함수가 최대가 되도록 하는 분포를 찾는 것이다.

##### ->  로지스틱 회귀모델의 로그우도함수

##### $ l(w,b) = \sum_{i=1} ^m lnp(y_i\mid x_i;w,b)$

> 즉, 각 샘플이 실제 레이블에 속할 확률이 높으면 높을수록 좋다는 의미이다.

##### * 식 정리

![](C:\Users\owner\AppData\Roaming\marktext\images\2022-01-15-16-44-25-image.png)

<img title="" src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-16-44-58-image.png" alt="" width="349">

**-> 결과**

<img src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-16-31-42-image.png" title="" alt="" width="290">

> 결론적으로, 처음의 식을 **최대화**하는 것이 위에 식을 **최소화**하는 것과 같다.
> 
> 위 식은 $ B$**의 고차 미분 가능 연속 컨벡스 함수**이다.

**-> 컨백스 함수의 최적해를 구하는 방법에는 두가지가 있다.**

##### 1. 경사하강법

##### 2. 뉴턴법

뉴턴법에 따르면, t+1번째 순서의 반복해의 공식은 다음과 같다.

<img src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-16-39-18-image.png" title="" alt="" width="324">

위 식의 B에 대한 1차, 2차 도함수는 다음과 같다.

<img src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-16-43-20-image.png" title="" alt="" width="421">

> **일차미분을 이용한 함수 최적화**는 현재 지점에서 일차미분(기울기)이 양수(f'>0)이면 x를 감소시키고, 음수(f'<0)면 x를 증가시키는 방식으로 f가 극소가 되는 지점을 찾는다.
> 
> <img src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-16-16-53-13-image.png" title="" alt="" width="406">





## 4. 선형 판별분석(LDA)

**선형 판별분석**이란 **데이터 분포**를 학습해 **결정경계**를 만들어 데이터를 분류하는 모델이다. 전통적인 선형학습법으로 기본 아이디어는 매우 단순하다. 우선 하나의 샘플을 하나의 직선에 투영시킨다. 

> * **같은 클래스 -> 최대한 가까이**
> 
> * **다른 클래스 -> 최대한 멀리**

<img title="" src="https://t1.daumcdn.net/cfile/tistory/992310465BF4E1E62C" alt="결국 고유값 문제로 수렴되는 선형판별분석(LDA) by bskyvision" width="243">

> 직선: $ y=w^Tx$
> 
> 클래스의 중심 투영점 : $ w^Tu_0$, $ w^Tu_1$
> 
> 클래스의 공분산 : $ w^T\sum _0w$, $ w^T\sum _1w$

### w 산출 방법

##### 1) 식 정리

위에서 언급한 두가지 목표를 달성하기 위해 다음 식을 최소화 해야한다.

$ J =$ $ \parallel w^Tu_0-w^Tu_1\parallel ^2_2\over w_T\sum _0w+w^T\sum _1w$

- 집단 내 산포행렬
  
  $ S_w = \sum _0+\sum _1$

- 집단 간 산포행렬
  
  $ S_b = (u_0-u_1)(u_0-u_1)^T$
  
  따라서 다시 식을 쓰면,

#### $ J=$ $ w^TS_bw\over w^TS_ww$

위 식이 **선형 판별분석이 최대화 하려는 목표**가 된다. 즉, $ S_b$와 $ S_w$ 의 **일반화된 레일리 몫**이다.

#### 2) w 계산

위 식에 따르면 분자와 분모가 모두 **w에 대한 이차 항**이다. 따라서 위 식의 해와 u의 **길이**는 무관하며 **방향**만 관련이 있다.

이 때 **일반성을 잃지 않고,** $ w^TS_ww = 1$ 이라면 위 식은 아래와 같다.

## $ S_bw = \lambda S_ww$

 ($ \lambda$: 라그랑주 곱셈자)

$ S_bw$의 **방향**이 $ (u_0-u_1)$이므로 $ S_bw=\lambda (u_0-u_1)$ 를 **대입**하면 아래와 같이 정리된다.

## $ w = S^{-1}_w(u_0-u_1)$



#### -> 선형 판별분석(LDA): 다항 분류 문제로 확장 가능

> 다중 분류 LDA는 샘플을 d'차원의 공간으로 투영한다. 이 때 d'은 d보다 많이 작으므로 이러한 다중 분류 LDA는 투영과정을 통해 샘플 포인트의 차원수를 줄이는 **지도적 차원 축소 기법**으로도 볼 수 있다. 





## 5. 다중 분류 학습

위와 같이 이진 분류 방법은 다중 분류 방법으로 확장될 수 있다. 그러나 많은 상황에서 다중 분류 문제를 **이중 분류 문제로 분해**하여 해결한다. 이중 분류 문제로 분해하여 학습기를 훈련시킨 다음 **하나의 분류기로 앙상블**하여 최종 다중 분류 결과를 얻는다.

- ### 분해 전략
  
  #### 1) 일대일 (OvO)
  
  1. N개의 클래스를 가진 데이터를 둘씩 분해
     
     **-> 총 N(N-1)/2개의 이진 분류 문제 생성**
     
     **-> 클래스 하나는 음성값, 하나는 양성값으로 분류**
  
     2.  새로운 샘플 데이터 모든 분류기에 테스트 진행

          3. N(N-1)/2개의 분류 결과 중 **가장 많이 예측된 클래스**가 **최종 분류 결괏값**

 

#### 2) 일대다 (OvR)

        1.  N개의 클래스를 매번 **한 분류만 양성값**으로 분류하고 나머지는 **모두 음성값으로             분류**

        2. 테스트시 **양성값으로 나오는 하나의 분류기**를 **최종 분류 결과값**으로 지정

            -> 만약 다수의 분류기에서 양성값으로 분류시, 각 **분류기의 예측 신뢰도 고려** 가장 큰                 신뢰도의 클레스 레이블을 **결괏값으로 지정**

> **OvO(일대일)와 OvR(일대다)의 비교**
> 
> - OvO는 N(N-1)/2개의 분류기를 훈련해야하는 반면, OvR은 N개만 훈련하면 된다.
> 
> - 그러므로 매모리 사용량과 훈련시간이 OvO가 더 크다.
> 
> - 그러나 OvO는 훈련에 두 클래스 데이터만 사용하는 반면 OvR은 모든 클래스 데이터를 사용하므로 클래스가 많은 경우 OvO의 훈련시간이 더 짧다.
> 
> - 성능은 비슷하다.

<img title="" src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-21-18-07-image.png" alt="" width="480">

#### 3) 다대다 (MvM)

        1. 몇 개의 클래스는 **양성값**에 몇 개의 클래스는 **음성값**으로 분류

            -> 이 때 분류 방법은 **특수한 설계**가 바탕이 된다.

            -> 가장 많이 사용되는 방법인 **오류 수정 출력 코드 (ECOC)**

> **오류 수정 출력 코드 (ECOC)**
> 
> - ECOC의 클래스 분류는 코딩 매트릭스에 따라 진행된다.
>   
>   -**이원코드** : 각 클래스를 음성과 양성으로 나눔
>   
>   -**삼원코드** : 각 클래스를 음성과 양성과 중성으로 나눔
> 
> - 각 분류기의 예측 결과는 **하나의 테스트 샘플 코드**로 연결된다.

            -> **M개의 분류기 생성**

        2. ECOC에 따라 만들어진 분류기 중, **테스트 샘플**의 코드와 **거리가 가장 짧은** 코드에             대응하는 클래스를 **예측 결괏값으로 지정**



## 6. 클래스 불균형 문제

앞서 나온 분류 학습법들은 **서로 다른 클래스의 훈련 샘플들의 수가 같다**는 전제를 가지고 있다. 그러나 현실에서는 그렇지 못한 경우도 있다. 이를 **클래스 불균형**이라고 한다.

> 예를 들어, 분해법으로 다중 분류 문제를 해결할 때 초기 훈련 세트의 서로다른 클래스 샘플 수 비율이 비슷하더라도 OvR, MvM 등의 방법을 거쳐 생성된 이분 분류 문제에서는 클래스 불균형 문제가 발생할 수 있다. 

선형 분류기의 관점에서 일반적으로 y>0.5 일 때 양성으로 판단하고, y<0.5  일 때 음성으로 판단한다. 임곗값을 0.5 로 설정했다는 것은 **분류기가 실제 양성값과 음성값이 될 가능성을 같게 본다**는 것이다. 따라서 분류기의 결정 규칙은 다음과 같다.

#### $ y\over 1-y$ $ >1$ 이면 양성값으로 예측

그러나 **훈련 데이터 세트 내의 양성, 음성값의 수가 다르다면** 관측 오즈는 $ m^+\over m^{-}$ 가 될 것이다. 우리는 일반적으로 훈련데이터 세트는 실제 샘플 전체에서의 비편향 추출 되었다고 가정하기 떄문에 관측 오즈는 실제 오즈를 나타낸다고 본다. 따라서 **분류기의 예측 오즈가 관측 오즈보다 높으면 양성값으로 판단**한다.

그러나 위 결정 규칙을 기반으로 해야하기 때문에 아래와 같은 **리스케일링 방법**을 사용한다.

#### <img src="file:///C:/Users/owner/AppData/Roaming/marktext/images/2022-01-15-21-20-02-image.png" title="" alt="" width="183">

그러나 리스케일링 방법은 실제로 적용하기 어렵다. 왜냐하면 '**훈련 세트는 실제 샘플 데이터의 비편향 추출**' 이라는 전제가 **성립하지 않을 때**가 있기 때문이다.

이럴 때 사용하는 **세가지 방법**이 있다.

##### 1) 언더 샘플링

    음성값이 많은 경우, **음성값을 제외**

> **음성샘플링 알고리즘 이지 앙상블**
> 
> : 음성 샘플을 여러개의 집합으로 나누어 서로 다른 학습기에 적용
> 
> : 중요한 정보를 손실하지 않게 해줌

##### 2) 오버 샘플링

    **양성값을 늘림**

> **오버샘플링 알고리즘 SMOTE**
> 
> : 훈련 세트 내의 양성 샘플들에 대해 보간법을 사용해 추가적인 양성 샘플 만들어낸다.

##### 3) 임곗값 이동

    원래 훈련 세트에 기반하여 학습을 진행하지만, 예측할 때는 **리스케일링 값**을 사용하는 방법
